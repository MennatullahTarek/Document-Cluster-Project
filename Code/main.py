# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lO8GwySYd03tuDySzpGemu_jqcmoDexW

$$ Document \space cluster \space project $$

---
# `#` Import Necessary Libraries
"""

import pandas as pd
from sklearn.datasets import fetch_20newsgroups

"""---
# `#` Import Modules
"""

from data_preprocessing import preprocess_text
from feature_extraction import extract_features_tfidf, extract_features_bert, scale_features
from clustering import apply_kmeans, reduce_dimensions
from evaluation import evaluate_clustering

"""---
# `#` Main function
"""

def main():
    # Load the 20 Newsgroups dataset
    categories = [
        'talk.religion.misc',
        'comp.graphics',
        'sci.space',
    ]

    print("Loading 20 newsgroups dataset for categories:")
    print(categories)

    news_data = fetch_20newsgroups(subset='all', categories=categories,
                                   shuffle=False, remove=('headers', 'footers', 'quotes'))

    # Load the Wikipedia Articles Dataset
    dataset = '/content/people_wiki.csv'
    people_wiki_df = pd.read_csv(dataset)

    # Preprocess the text data from both datasets
    processed_news_data = [preprocess_text(doc) for doc in news_data.data]
    processed_wiki_data = [preprocess_text(doc) for doc in people_wiki_df['text']]

    # Feature extraction using BERT for both datasets
    features_news = extract_features_bert(processed_news_data)
    features_wiki = extract_features_bert(processed_wiki_data)

    # Scale features for both datasets
    scaled_features_news = scale_features(features_news)
    scaled_features_wiki = scale_features(features_wiki)

    # Reduce dimensions for both datasets
    reduced_features_news = reduce_dimensions(scaled_features_news)
    reduced_features_wiki = reduce_dimensions(scaled_features_wiki)

    # Apply KMeans clustering on both datasets
    labels_news = apply_kmeans(reduced_features_news, n_clusters=3)
    labels_wiki = apply_kmeans(reduced_features_wiki, n_clusters=3)

    # Evaluate clustering for both datasets
    score_news = evaluate_clustering(labels_news, reduced_features_news)
    score_wiki = evaluate_clustering(labels_wiki, reduced_features_wiki)

    print(f"Silhouette Score for 20 Newsgroups Dataset: {score_news}")
    print(f"Silhouette Score for Wikipedia Dataset: {score_wiki}")

if __name__ == "__main__":
    main()